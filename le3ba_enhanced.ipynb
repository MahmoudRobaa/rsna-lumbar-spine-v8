{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 71549,
     "databundleVersionId": 8561470,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# RSNA 2024 Lumbar Spine Degenerative Classification\n## Version 4\n\n### Key Fixes in v4:\n1. **Class weights calculated from ORIGINAL data** (not oversampled!)\n2. **Model selection based on Balanced Accuracy** (not val_loss)\n3. **Increased focal_gamma to 3.5** (from 2.0)\n4. **Added minimum recall threshold** for model saving",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport cv2\nimport glob\nimport pydicom\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedGroupKFold, train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\nfrom collections import Counter",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:15.819584Z",
     "iopub.execute_input": "2026-02-08T05:06:15.820184Z",
     "iopub.status.idle": "2026-02-08T05:06:17.913483Z",
     "shell.execute_reply.started": "2026-02-08T05:06:15.820141Z",
     "shell.execute_reply": "2026-02-08T05:06:17.912705Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.amp import autocast, GradScaler\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:17.914949Z",
     "iopub.execute_input": "2026-02-08T05:06:17.915468Z",
     "iopub.status.idle": "2026-02-08T05:06:25.964003Z",
     "shell.execute_reply.started": "2026-02-08T05:06:17.915443Z",
     "shell.execute_reply": "2026-02-08T05:06:25.963415Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# --- CONFIGURATION ---\nCONFIG = {\n    'seed': 42,\n    'img_size': 256,\n    'seq_length': 7,\n    'batch_size': 8,\n    'epochs': 25,\n    'learning_rate': 3e-4,\n    'backbone_lr': 3e-5,\n    'weight_decay': 0.05,\n    'patience': 10,  # Increased patience since we use balanced_acc\n    'num_folds': 5,\n    'train_folds': [0],\n    'focal_gamma': 3.5,  # INCREASED from 2.0 to focus more on hard examples\n    'label_smoothing': 0.1,\n    'dropout': 0.4,\n    'num_attention_heads': 4,\n    'warmup_epochs': 2,\n    'oversample_strategy': 'progressive',  # 'progressive', 'smote_like', or 'balanced'\n    'min_minority_recall': 0.20,  # Minimum recall for minority classes to save model\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'target_condition': 'spinal_canal_stenosis',\n    'target_series': 'Sagittal T2/STIR'\n}",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:25.964839Z",
     "iopub.execute_input": "2026-02-08T05:06:25.965286Z",
     "iopub.status.idle": "2026-02-08T05:06:26.218487Z",
     "shell.execute_reply.started": "2026-02-08T05:06:25.965261Z",
     "shell.execute_reply": "2026-02-08T05:06:26.217708Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:26.220575Z",
     "iopub.execute_input": "2026-02-08T05:06:26.220932Z",
     "iopub.status.idle": "2026-02-08T05:06:26.235335Z",
     "shell.execute_reply.started": "2026-02-08T05:06:26.220909Z",
     "shell.execute_reply": "2026-02-08T05:06:26.234645Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "seed_everything(CONFIG['seed'])\nprint(f\"‚úÖ Device: {CONFIG['device']}\")\nprint(f\"   Oversample Strategy: {CONFIG['oversample_strategy']}\")\nprint(f\"   Focal Gamma: {CONFIG['focal_gamma']} (Higher = more focus on hard examples)\")",
   "metadata": {
    "lines_to_next_cell": 2,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:26.236069Z",
     "iopub.execute_input": "2026-02-08T05:06:26.236284Z",
     "iopub.status.idle": "2026-02-08T05:06:26.252297Z",
     "shell.execute_reply.started": "2026-02-08T05:06:26.236264Z",
     "shell.execute_reply": "2026-02-08T05:06:26.251745Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Data Loading",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- PATHS ---\nDATA_ROOT = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\"\nTRAIN_IMAGES = os.path.join(DATA_ROOT, \"train_images\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:26.253169Z",
     "iopub.execute_input": "2026-02-08T05:06:26.253408Z",
     "iopub.status.idle": "2026-02-08T05:06:26.256972Z",
     "shell.execute_reply.started": "2026-02-08T05:06:26.253389Z",
     "shell.execute_reply": "2026-02-08T05:06:26.256291Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_train = pd.read_csv(f\"{DATA_ROOT}/train.csv\")\ndf_coords = pd.read_csv(f\"{DATA_ROOT}/train_label_coordinates.csv\")\ndf_desc = pd.read_csv(f\"{DATA_ROOT}/train_series_descriptions.csv\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:28.488834Z",
     "iopub.execute_input": "2026-02-08T05:06:28.489159Z",
     "iopub.status.idle": "2026-02-08T05:06:28.629935Z",
     "shell.execute_reply.started": "2026-02-08T05:06:28.489131Z",
     "shell.execute_reply": "2026-02-08T05:06:28.629096Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Clean & Merge\ndf_train.columns = [col.lower().replace('/', '_') for col in df_train.columns]\ncondition_cols = [c for c in df_train.columns if c != 'study_id']\ndf_labels = pd.melt(df_train, id_vars=['study_id'], value_vars=condition_cols,\n                    var_name='condition_level', value_name='severity')\ndf_labels = df_labels.dropna(subset=['severity'])\ndf_labels['severity'] = df_labels['severity'].astype(str).str.lower().str.replace('/', '_')",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:28.996299Z",
     "iopub.execute_input": "2026-02-08T05:06:28.996763Z",
     "iopub.status.idle": "2026-02-08T05:06:29.040858Z",
     "shell.execute_reply.started": "2026-02-08T05:06:28.996739Z",
     "shell.execute_reply": "2026-02-08T05:06:29.040303Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def extract_meta(val):\n    parts = val.split('_')\n    level = parts[-2] + '_' + parts[-1]\n    condition = '_'.join(parts[:-2])\n    return condition, level",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:29.202625Z",
     "iopub.execute_input": "2026-02-08T05:06:29.203143Z",
     "iopub.status.idle": "2026-02-08T05:06:29.207047Z",
     "shell.execute_reply.started": "2026-02-08T05:06:29.20312Z",
     "shell.execute_reply": "2026-02-08T05:06:29.206281Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_labels[['base_condition', 'level_str']] = df_labels['condition_level'].apply(lambda x: pd.Series(extract_meta(x)))\nseverity_map = {'normal_mild': 0, 'moderate': 1, 'severe': 2}\ndf_labels['label'] = df_labels['severity'].map(severity_map)\ndf_labels = df_labels.dropna(subset=['label'])\ndf_labels['label'] = df_labels['label'].astype(int)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:29.461535Z",
     "iopub.execute_input": "2026-02-08T05:06:29.462076Z",
     "iopub.status.idle": "2026-02-08T05:06:33.498634Z",
     "shell.execute_reply.started": "2026-02-08T05:06:29.462036Z",
     "shell.execute_reply": "2026-02-08T05:06:33.498069Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_coords = df_coords.merge(df_desc, on=['study_id', 'series_id'], how='left')\ndf_coords['condition'] = df_coords['condition'].str.lower().str.replace(' ', '_')\ndf_coords['level'] = df_coords['level'].str.lower().str.replace('/', '_')\ndf_coords['condition_level'] = df_coords['condition'] + '_' + df_coords['level']",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:33.499961Z",
     "iopub.execute_input": "2026-02-08T05:06:33.500701Z",
     "iopub.status.idle": "2026-02-08T05:06:33.563447Z",
     "shell.execute_reply.started": "2026-02-08T05:06:33.500676Z",
     "shell.execute_reply": "2026-02-08T05:06:33.562679Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_model = df_labels[df_labels['base_condition'] == CONFIG['target_condition']].copy()\ndf_coords_filt = df_coords[(df_coords['condition'] == CONFIG['target_condition']) & \n                           (df_coords['series_description'] == CONFIG['target_series'])]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:33.564307Z",
     "iopub.execute_input": "2026-02-08T05:06:33.56453Z",
     "iopub.status.idle": "2026-02-08T05:06:33.585243Z",
     "shell.execute_reply.started": "2026-02-08T05:06:33.564509Z",
     "shell.execute_reply": "2026-02-08T05:06:33.584495Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_final = df_model.merge(df_coords_filt[['study_id', 'condition_level', 'series_id', 'instance_number', 'x', 'y']],\n                          on=['study_id', 'condition_level'], how='inner')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:33.586462Z",
     "iopub.execute_input": "2026-02-08T05:06:33.586695Z",
     "iopub.status.idle": "2026-02-08T05:06:33.599531Z",
     "shell.execute_reply.started": "2026-02-08T05:06:33.586673Z",
     "shell.execute_reply": "2026-02-08T05:06:33.598675Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Filter valid files\nvalid_rows = []\nfor index, row in tqdm(df_final.iterrows(), total=len(df_final), desc=\"Checking Files\"):\n    path = f\"{TRAIN_IMAGES}/{row['study_id']}/{row['series_id']}/{int(row['instance_number'])}.dcm\"\n    if os.path.exists(path):\n        valid_rows.append(row)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:33.834874Z",
     "iopub.execute_input": "2026-02-08T05:06:33.835184Z",
     "iopub.status.idle": "2026-02-08T05:06:57.669146Z",
     "shell.execute_reply.started": "2026-02-08T05:06:33.835158Z",
     "shell.execute_reply": "2026-02-08T05:06:57.668555Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_final = pd.DataFrame(valid_rows).reset_index(drop=True)\nlevel_map = {'l1_l2': 0, 'l2_l3': 1, 'l3_l4': 2, 'l4_l5': 3, 'l5_s1': 4}\ndf_final['level_idx'] = df_final['level_str'].map(level_map)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:57.670298Z",
     "iopub.execute_input": "2026-02-08T05:06:57.670531Z",
     "iopub.status.idle": "2026-02-08T05:06:57.76562Z",
     "shell.execute_reply.started": "2026-02-08T05:06:57.67051Z",
     "shell.execute_reply": "2026-02-08T05:06:57.765062Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(f\"\\n‚úÖ Data Ready: {len(df_final)} samples\")\nprint(f\"   Class Distribution: {df_final['label'].value_counts().sort_index().to_dict()}\")\nclass_counts = df_final['label'].value_counts().sort_index()\nfor i, count in enumerate(class_counts):\n    pct = count / len(df_final) * 100\n    print(f\"   Class {i}: {count} samples ({pct:.1f}%)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:06:57.766503Z",
     "iopub.execute_input": "2026-02-08T05:06:57.766789Z",
     "iopub.status.idle": "2026-02-08T05:06:57.775547Z",
     "shell.execute_reply.started": "2026-02-08T05:06:57.766757Z",
     "shell.execute_reply": "2026-02-08T05:06:57.774992Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Balanced Oversampling Function",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_stratified_balanced_df(df, strategy='progressive', random_state=42):\n    \"\"\"\n    Progressive balancing that considers both label and level together.\n    Adds augmentation variant tracking for diversity.\n    \"\"\"\n    np.random.seed(random_state)\n    grouped = df.groupby(['level_idx', 'label'])\n    balanced_dfs = []\n    \n    print(\"\\nüìä Stratified Sampling Details:\")\n    \n    for (level, label), group_df in grouped:\n        group_df = group_df.copy()\n        group_df['is_oversampled'] = False\n        group_df['aug_variant'] = 0\n        current_count = len(group_df)\n        level_counts = df[df['level_idx'] == level]['label'].value_counts()\n        \n        if strategy == 'progressive':\n            target_count = int(level_counts.median() * (1 + 0.3 * label))\n        elif strategy == 'smote_like':\n            if current_count < level_counts.median():\n                target_count = int(level_counts.median() * 0.8)\n            else:\n                target_count = current_count\n        elif strategy == 'balanced':\n            target_count = level_counts.max()\n        else:\n            target_count = current_count\n        \n        samples_needed = target_count - current_count\n        \n        if samples_needed > 0:\n            oversample_indices = np.random.choice(group_df.index, size=samples_needed, replace=True)\n            oversampled_df = df.loc[oversample_indices].copy()\n            oversampled_df['is_oversampled'] = True\n            oversampled_df['aug_variant'] = np.random.randint(0, 4, size=len(oversampled_df))\n            print(f\"   Level {level}, Label {label}: {current_count} ‚Üí {target_count} (+{samples_needed})\")\n            balanced_dfs.append(group_df)\n            balanced_dfs.append(oversampled_df)\n        else:\n            print(f\"   Level {level}, Label {label}: {current_count} (no oversampling)\")\n            balanced_dfs.append(group_df)\n    \n    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n    return balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:00.738559Z",
     "iopub.execute_input": "2026-02-08T05:07:00.739079Z",
     "iopub.status.idle": "2026-02-08T05:07:00.745767Z",
     "shell.execute_reply.started": "2026-02-08T05:07:00.739052Z",
     "shell.execute_reply": "2026-02-08T05:07:00.745162Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Test the function\nprint(\"\\nüìä Before Oversampling:\")\nprint(df_final['label'].value_counts().sort_index())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:03.222262Z",
     "iopub.execute_input": "2026-02-08T05:07:03.222809Z",
     "iopub.status.idle": "2026-02-08T05:07:03.227992Z",
     "shell.execute_reply.started": "2026-02-08T05:07:03.22278Z",
     "shell.execute_reply": "2026-02-08T05:07:03.227419Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "balanced_test = create_stratified_balanced_df(df_final, strategy='balanced')\nprint(\"\\nüìä After Balanced Oversampling:\")\nprint(balanced_test['label'].value_counts().sort_index())\nprint(f\"   Total samples: {len(df_final)} ‚Üí {len(balanced_test)}\")\nprint(f\"   Oversampled: {balanced_test['is_oversampled'].sum()} samples\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:03.641409Z",
     "iopub.execute_input": "2026-02-08T05:07:03.642137Z",
     "iopub.status.idle": "2026-02-08T05:07:03.668431Z",
     "shell.execute_reply.started": "2026-02-08T05:07:03.642107Z",
     "shell.execute_reply": "2026-02-08T05:07:03.667781Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Dataset with Adaptive Augmentation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class RSNASequenceDataset(Dataset):\n    def __init__(self, df, seq_length=7, img_size=256, transform=None, \n                 strong_transform=None, is_training=False):\n        self.df = df.reset_index(drop=True)\n        self.seq_length = seq_length\n        self.img_size = img_size\n        self.transform = transform\n        self.strong_transform = strong_transform  # For oversampled data\n        self.is_training = is_training\n        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def load_dicom(self, path):\n        try:\n            dcm = pydicom.dcmread(path)\n            img = dcm.pixel_array.astype(np.float32)\n            if img.max() > img.min():\n                img = (img - img.min()) / (img.max() - img.min()) * 255.0\n            else:\n                img = np.zeros_like(img)\n            img = img.astype(np.uint8)\n            img = self.clahe.apply(img)\n            return img\n        except:\n            return np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        center_inst = int(row['instance_number'])\n        study_path = f\"{TRAIN_IMAGES}/{row['study_id']}/{row['series_id']}\"\n        cx, cy = int(row['x']), int(row['y'])\n        \n        # Check if this sample is oversampled (needs stronger augmentation)\n        is_oversampled = row.get('is_oversampled', False)\n        aug_variant = row.get('aug_variant', 0)\n        \n        # Set different random seed based on variant for diversity\n        if is_oversampled and self.is_training:\n            np.random.seed(idx * 1000 + int(aug_variant))\n            random.seed(idx * 1000 + int(aug_variant))\n        \n        start = center_inst - (self.seq_length // 2)\n        indices = [start + i for i in range(self.seq_length)]\n        \n        images_list = []\n        for inst in indices:\n            path = os.path.join(study_path, f\"{inst}.dcm\")\n            if os.path.exists(path):\n                img = self.load_dicom(path)\n            else:\n                img = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n            \n            h, w = img.shape\n            crop_size = self.img_size // 2 \n            x1 = max(0, cx - crop_size)\n            y1 = max(0, cy - crop_size)\n            x2 = min(w, cx + crop_size)\n            y2 = min(h, cy + crop_size)\n            crop = img[y1:y2, x1:x2]\n            \n            if crop.size == 0:\n                crop = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n            else:\n                crop = cv2.resize(crop, (self.img_size, self.img_size))\n            \n            crop = cv2.cvtColor(crop, cv2.COLOR_GRAY2RGB)\n            \n            # Use stronger augmentation for oversampled data\n            if self.is_training and is_oversampled and self.strong_transform:\n                res = self.strong_transform(image=crop)\n            elif self.transform:\n                res = self.transform(image=crop)\n            else:\n                res = {'image': torch.tensor(crop).permute(2, 0, 1).float() / 255.0}\n            \n            crop_tensor = res['image']\n            images_list.append(crop_tensor)\n            \n        sequence = torch.stack(images_list, dim=0)\n        label = torch.tensor(row['label'], dtype=torch.long)\n        level_idx = torch.tensor(row['level_idx'], dtype=torch.long)\n        \n        return sequence, label, level_idx",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:06.365332Z",
     "iopub.execute_input": "2026-02-08T05:07:06.365632Z",
     "iopub.status.idle": "2026-02-08T05:07:06.379166Z",
     "shell.execute_reply.started": "2026-02-08T05:07:06.365601Z",
     "shell.execute_reply": "2026-02-08T05:07:06.378489Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Augmentation Pipelines (Normal + Strong)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Medical imaging-appropriate augmentation (normal)\ntrain_aug = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=5, \n                       border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n    A.GaussNoise(var_limit=(5.0, 20.0), p=0.2),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:11.325391Z",
     "iopub.execute_input": "2026-02-08T05:07:11.325821Z",
     "iopub.status.idle": "2026-02-08T05:07:11.338123Z",
     "shell.execute_reply.started": "2026-02-08T05:07:11.325784Z",
     "shell.execute_reply": "2026-02-08T05:07:11.337144Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Medical imaging-appropriate augmentation (strong for oversampled)\nstrong_aug = A.Compose([\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=8,\n                       border_mode=cv2.BORDER_CONSTANT, value=0, p=0.7),\n    A.ElasticTransform(alpha=1, sigma=50, alpha_affine=20, p=0.3),\n    A.OneOf([\n        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.0),\n        A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n        A.CLAHE(clip_limit=4.0, p=1.0),\n    ], p=0.8),\n    A.OneOf([\n        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n        A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n    ], p=0.3),\n    A.GridDistortion(num_steps=5, distort_limit=0.1, p=0.3),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2(),\n])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:11.613253Z",
     "iopub.execute_input": "2026-02-08T05:07:11.613545Z",
     "iopub.status.idle": "2026-02-08T05:07:11.624397Z",
     "shell.execute_reply.started": "2026-02-08T05:07:11.613519Z",
     "shell.execute_reply": "2026-02-08T05:07:11.62373Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "val_aug = A.Compose([\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:11.861522Z",
     "iopub.execute_input": "2026-02-08T05:07:11.861754Z",
     "iopub.status.idle": "2026-02-08T05:07:11.866077Z",
     "shell.execute_reply.started": "2026-02-08T05:07:11.861732Z",
     "shell.execute_reply": "2026-02-08T05:07:11.865322Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"‚úÖ Dual augmentation pipeline (ENHANCED):\")\nprint(\"   - Normal: for original samples\")\nprint(\"   - Strong: for oversampled minority class samples (more aggressive)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:13.076459Z",
     "iopub.execute_input": "2026-02-08T05:07:13.076931Z",
     "iopub.status.idle": "2026-02-08T05:07:13.081041Z",
     "shell.execute_reply.started": "2026-02-08T05:07:13.076904Z",
     "shell.execute_reply": "2026-02-08T05:07:13.080439Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Model Architecture",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SpineSeqAttention(nn.Module):\n    def __init__(self, num_classes=3, hidden_dim=256, lstm_layers=2, \n                 num_heads=4, dropout=0.4, num_levels=5):\n        super(SpineSeqAttention, self).__init__()\n        \n        effnet = models.efficientnet_v2_s(weights='IMAGENET1K_V1')\n        self.backbone = nn.Sequential(*list(effnet.children())[:-1]) \n        self.feature_dim = 1280 \n        \n        self.feature_proj = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(self.feature_dim, hidden_dim * 2),\n            nn.LayerNorm(hidden_dim * 2),\n            nn.GELU()\n        )\n        \n        self.lstm = nn.LSTM(\n            input_size=hidden_dim * 2, \n            hidden_size=hidden_dim, \n            num_layers=lstm_layers, \n            batch_first=True, \n            bidirectional=True, \n            dropout=dropout if lstm_layers > 1 else 0\n        )\n        \n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim * 2,\n            num_heads=num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.level_embedding = nn.Embedding(num_levels, 64)\n        \n        self.classifier = nn.Sequential(\n            nn.LayerNorm(hidden_dim * 2 + 64),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim * 2 + 64, 128),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, num_classes)\n        )\n        \n    def forward(self, x, level_idx=None):\n        b, s, c, h, w = x.size()\n        x = x.view(b * s, c, h, w)\n        \n        features = self.backbone(x)\n        features = features.view(b, s, -1)\n        features = self.feature_proj(features)\n        \n        lstm_out, _ = self.lstm(features)\n        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n        context = attn_out.mean(dim=1)\n        \n        if level_idx is not None:\n            level_feat = self.level_embedding(level_idx)\n            context = torch.cat([context, level_feat], dim=-1)\n        else:\n            context = torch.cat([context, torch.zeros(b, 64, device=x.device)], dim=-1)\n        \n        out = self.classifier(context)\n        avg_attn = attn_weights.mean(dim=1)\n        \n        return out, avg_attn",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:19.340709Z",
     "iopub.execute_input": "2026-02-08T05:07:19.341354Z",
     "iopub.status.idle": "2026-02-08T05:07:19.349932Z",
     "shell.execute_reply.started": "2026-02-08T05:07:19.341326Z",
     "shell.execute_reply": "2026-02-08T05:07:19.349308Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"‚úÖ Model architecture loaded\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:21.702556Z",
     "iopub.execute_input": "2026-02-08T05:07:21.703318Z",
     "iopub.status.idle": "2026-02-08T05:07:21.707305Z",
     "shell.execute_reply.started": "2026-02-08T05:07:21.70329Z",
     "shell.execute_reply": "2026-02-08T05:07:21.706569Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Focal Loss + Early Stopping",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.1, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.label_smoothing = label_smoothing\n        self.reduction = reduction\n        \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(\n            inputs, targets, \n            weight=self.alpha, \n            reduction='none', \n            label_smoothing=self.label_smoothing\n        )\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n        \n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        return focal_loss",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:24.183676Z",
     "iopub.execute_input": "2026-02-08T05:07:24.183997Z",
     "iopub.status.idle": "2026-02-08T05:07:24.189205Z",
     "shell.execute_reply.started": "2026-02-08T05:07:24.183962Z",
     "shell.execute_reply": "2026-02-08T05:07:24.188576Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "class EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.001, mode='max'):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.mode = mode  # 'max' for balanced accuracy, 'min' for loss\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        \n    def __call__(self, val_score):\n        if self.best_score is None:\n            self.best_score = val_score\n            return False\n        \n        if self.mode == 'max':\n            # Higher is better (e.g., balanced accuracy)\n            if val_score > self.best_score + self.min_delta:\n                self.best_score = val_score\n                self.counter = 0\n                return False\n        else:\n            # Lower is better (e.g., loss)\n            if val_score < self.best_score - self.min_delta:\n                self.best_score = val_score\n                self.counter = 0\n                return False\n        \n        self.counter += 1\n        if self.counter >= self.patience:\n            self.early_stop = True\n            return True\n        return False",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:24.557732Z",
     "iopub.execute_input": "2026-02-08T05:07:24.558346Z",
     "iopub.status.idle": "2026-02-08T05:07:24.564302Z",
     "shell.execute_reply.started": "2026-02-08T05:07:24.558318Z",
     "shell.execute_reply": "2026-02-08T05:07:24.563586Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"‚úÖ Focal Loss + Early Stopping (uses balanced accuracy)\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:26.035615Z",
     "iopub.execute_input": "2026-02-08T05:07:26.036144Z",
     "iopub.status.idle": "2026-02-08T05:07:26.039866Z",
     "shell.execute_reply.started": "2026-02-08T05:07:26.036116Z",
     "shell.execute_reply": "2026-02-08T05:07:26.03934Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Training Function with Per-Class Metrics (FIXED!)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def compute_per_class_metrics(preds, labels, num_classes=3):\n    \"\"\"Compute per-class accuracy (recall)\"\"\"\n    metrics = {}\n    for c in range(num_classes):\n        mask = (labels == c)\n        if mask.sum() > 0:\n            correct = ((preds == c) & mask).sum()\n            metrics[f'class_{c}_recall'] = correct / mask.sum()\n        else:\n            metrics[f'class_{c}_recall'] = 0.0\n    return metrics",
   "metadata": {
    "lines_to_next_cell": 1,
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:27.681485Z",
     "iopub.execute_input": "2026-02-08T05:07:27.681963Z",
     "iopub.status.idle": "2026-02-08T05:07:27.686413Z",
     "shell.execute_reply.started": "2026-02-08T05:07:27.681936Z",
     "shell.execute_reply": "2026-02-08T05:07:27.685738Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def train_one_fold(model, train_loader, val_loader, fold, config, original_class_counts):\n    \"\"\"\n    FIXED Training function:\n    1. Uses ORIGINAL class counts for loss weights (not oversampled!)\n    2. Uses BALANCED ACCURACY for model selection (not val_loss!)\n    3. Tracks minimum minority recall threshold\n    \"\"\"\n    \n    # ========================================\n    # FIX #1: Use ORIGINAL class counts for loss weights!\n    # ========================================\n    # After oversampling, all classes have equal counts, so weights would be ~1.0 each\n    # This negates the benefit of weighted loss!\n    # We use the ORIGINAL (pre-oversampling) counts instead\n    class_weights = 1. / (original_class_counts + 1e-6)\n    class_weights = class_weights / class_weights.sum() * 3\n    loss_weights = torch.FloatTensor(class_weights).to(config['device'])\n    \n    print(f\"\\n   üìä Class weights from ORIGINAL data: {class_weights}\")\n    print(f\"      (Class 0: {class_weights[0]:.3f}, Class 1: {class_weights[1]:.3f}, Class 2: {class_weights[2]:.3f})\")\n    \n    criterion = FocalLoss(\n        alpha=loss_weights, \n        gamma=config['focal_gamma'], \n        label_smoothing=config['label_smoothing']\n    )\n    \n    optimizer = optim.AdamW([\n        {'params': model.backbone.parameters(), 'lr': config['backbone_lr']},\n        {'params': model.feature_proj.parameters(), 'lr': config['learning_rate']},\n        {'params': model.lstm.parameters(), 'lr': config['learning_rate']},\n        {'params': model.attention.parameters(), 'lr': config['learning_rate']},\n        {'params': model.level_embedding.parameters(), 'lr': config['learning_rate']},\n        {'params': model.classifier.parameters(), 'lr': config['learning_rate']}\n    ], weight_decay=config['weight_decay'])\n    \n    warmup_steps = config['warmup_epochs'] * len(train_loader)\n    total_steps = config['epochs'] * len(train_loader)\n    \n    def lr_lambda(step):\n        if step < warmup_steps:\n            return step / warmup_steps\n        else:\n            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n            return 0.5 * (1 + np.cos(np.pi * progress))\n    \n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    scaler = GradScaler('cuda')\n    \n    # ========================================\n    # FIX #2: Use BALANCED ACCURACY for early stopping!\n    # ========================================\n    early_stopping = EarlyStopping(patience=config['patience'], min_delta=0.005, mode='max')\n    \n    best_balanced_acc = 0.0\n    history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': [], 'balanced_acc': [],\n        'class_0_recall': [], 'class_1_recall': [], 'class_2_recall': []\n    }\n    \n    print(f\"\\nüöÄ Training Fold {fold+1}/{config['num_folds']}\")\n    print(f\"   Train: {len(train_loader.dataset)}, Val: {len(val_loader.dataset)}\")\n    print(f\"   Focal Gamma: {config['focal_gamma']}\")\n    print(f\"   ‚ö†Ô∏è  Model saved based on BALANCED ACCURACY (not val_loss!)\")\n    \n    for epoch in range(config['epochs']):\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n        \n        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n        \n        for images, labels, level_idx in loop:\n            images = images.to(config['device'])\n            labels = labels.to(config['device'])\n            level_idx = level_idx.to(config['device'])\n            \n            optimizer.zero_grad()\n            \n            with autocast('cuda'):\n                outputs, _ = model(images, level_idx)\n                loss = criterion(outputs, labels)\n                \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            \n            train_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            train_correct += (predicted == labels).sum().item()\n            train_total += labels.size(0)\n            \n            loop.set_postfix(\n                loss=f\"{train_loss/(loop.n+1):.4f}\", \n                acc=f\"{100*train_correct/train_total:.1f}%\",\n                lr=f\"{optimizer.param_groups[0]['lr']:.2e}\"\n            )\n        \n        train_epoch_loss = train_loss / len(train_loader)\n        train_acc = train_correct / train_total\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        all_preds = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for images, labels, level_idx in val_loader:\n                images = images.to(config['device'])\n                labels = labels.to(config['device'])\n                level_idx = level_idx.to(config['device'])\n                \n                with autocast('cuda'):\n                    outputs, _ = model(images, level_idx)\n                    loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                val_correct += (predicted == labels).sum().item()\n                val_total += labels.size(0)\n                \n                all_preds.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n        \n        val_epoch_loss = val_loss / len(val_loader)\n        val_acc = val_correct / val_total\n        \n        # Compute per-class recall\n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        per_class = compute_per_class_metrics(all_preds, all_labels)\n        \n        # ========================================\n        # FIX #3: Compute and track BALANCED ACCURACY\n        # ========================================\n        balanced_acc = (per_class['class_0_recall'] + \n                       per_class['class_1_recall'] + \n                       per_class['class_2_recall']) / 3\n        \n        history['train_loss'].append(train_epoch_loss)\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_epoch_loss)\n        history['val_acc'].append(val_acc)\n        history['balanced_acc'].append(balanced_acc)\n        history['class_0_recall'].append(per_class['class_0_recall'])\n        history['class_1_recall'].append(per_class['class_1_recall'])\n        history['class_2_recall'].append(per_class['class_2_recall'])\n        \n        print(f\"üìä Train Loss: {train_epoch_loss:.4f} | Train Acc: {100*train_acc:.1f}% | \"\n              f\"Val Loss: {val_epoch_loss:.4f} | Val Acc: {100*val_acc:.1f}%\")\n        print(f\"   Per-class Recall: Normal={100*per_class['class_0_recall']:.1f}%, \"\n              f\"Moderate={100*per_class['class_1_recall']:.1f}%, \"\n              f\"Severe={100*per_class['class_2_recall']:.1f}%\")\n        print(f\"   üéØ Balanced Accuracy: {100*balanced_acc:.1f}%\")\n        \n        # ========================================\n        # FIX #4: Save based on BALANCED ACCURACY!\n        # ========================================\n        # Also check minimum recall threshold for minority classes\n        min_minority_recall = min(per_class['class_1_recall'], per_class['class_2_recall'])\n        \n        if balanced_acc > best_balanced_acc and min_minority_recall >= config.get('min_minority_recall', 0.1):\n            best_balanced_acc = balanced_acc\n            torch.save(model.state_dict(), f\"best_model_fold{fold}.pth\")\n            print(f\"‚úÖ Best Model Saved! (Balanced Acc: {100*balanced_acc:.1f}%, \"\n                  f\"Min Minority Recall: {100*min_minority_recall:.1f}%)\")\n        \n        if early_stopping(balanced_acc):\n            print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1} (balanced acc not improving)\")\n            break\n    \n    model.load_state_dict(torch.load(f\"best_model_fold{fold}.pth\"))\n    \n    return model, history, best_balanced_acc",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:30.913656Z",
     "iopub.execute_input": "2026-02-08T05:07:30.914221Z",
     "iopub.status.idle": "2026-02-08T05:07:30.93139Z",
     "shell.execute_reply.started": "2026-02-08T05:07:30.914186Z",
     "shell.execute_reply": "2026-02-08T05:07:30.930717Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Training with Balanced Oversampling (FIXED!)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "kfold = StratifiedGroupKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:35.38758Z",
     "iopub.execute_input": "2026-02-08T05:07:35.388243Z",
     "iopub.status.idle": "2026-02-08T05:07:35.391646Z",
     "shell.execute_reply.started": "2026-02-08T05:07:35.388211Z",
     "shell.execute_reply": "2026-02-08T05:07:35.391088Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "fold_results = []",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:35.955068Z",
     "iopub.execute_input": "2026-02-08T05:07:35.955697Z",
     "iopub.status.idle": "2026-02-08T05:07:35.958976Z",
     "shell.execute_reply.started": "2026-02-08T05:07:35.955672Z",
     "shell.execute_reply": "2026-02-08T05:07:35.958244Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "for fold, (train_idx, val_idx) in enumerate(kfold.split(df_final, df_final['label'], df_final['study_id'])):\n    if fold not in CONFIG['train_folds']:\n        continue\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"FOLD {fold + 1}/{CONFIG['num_folds']}\")\n    print(f\"{'='*60}\")\n    \n    # Get original train/val splits\n    train_df_original = df_final.iloc[train_idx].reset_index(drop=True)\n    val_df = df_final.iloc[val_idx].reset_index(drop=True)\n    \n    # ========================================\n    # Get ORIGINAL class counts BEFORE oversampling!\n    # ========================================\n    original_class_counts = np.bincount(train_df_original['label'].values, minlength=3)\n    print(f\"\\nüìä ORIGINAL Class Distribution (for loss weights):\")\n    for i, count in enumerate(original_class_counts):\n        print(f\"   Class {i}: {count} samples\")\n    \n    # Apply oversampling to training data ONLY\n    train_df = create_stratified_balanced_df(\n        train_df_original, \n        strategy=CONFIG['oversample_strategy'],\n        random_state=CONFIG['seed'] + fold\n    )\n    \n    print(f\"\\nüìä Training Data (after oversampling):\")\n    print(f\"   Original: {len(train_df_original)} samples\")\n    print(f\"   After Oversampling: {len(train_df)} samples\")\n    print(f\"   Class distribution: {train_df['label'].value_counts().sort_index().to_dict()}\")\n    \n    train_dataset = RSNASequenceDataset(\n        train_df, \n        seq_length=CONFIG['seq_length'], \n        img_size=CONFIG['img_size'], \n        transform=train_aug,\n        strong_transform=strong_aug,  # For oversampled data\n        is_training=True\n    )\n    \n    # Validation uses original (non-oversampled) data\n    val_df['is_oversampled'] = False  # Add column for compatibility\n    val_dataset = RSNASequenceDataset(\n        val_df, \n        seq_length=CONFIG['seq_length'], \n        img_size=CONFIG['img_size'], \n        transform=val_aug,\n        is_training=False\n    )\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=CONFIG['batch_size'], \n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=CONFIG['batch_size'], \n        shuffle=False, \n        num_workers=2,\n        pin_memory=True\n    )\n    \n    model = SpineSeqAttention(\n        num_classes=3,\n        num_heads=CONFIG['num_attention_heads'],\n        dropout=CONFIG['dropout']\n    ).to(CONFIG['device'])\n    \n    # Pass original class counts to training function!\n    model, history, best_balanced_acc = train_one_fold(\n        model, train_loader, val_loader, fold, CONFIG, original_class_counts\n    )\n    \n    fold_results.append({\n        'fold': fold,\n        'best_balanced_acc': best_balanced_acc,\n        'history': history\n    })\n    \n    print(f\"\\n‚úÖ Fold {fold+1} Complete | Best Balanced Acc: {100*best_balanced_acc:.1f}%\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-02-08T05:07:37.201966Z",
     "iopub.execute_input": "2026-02-08T05:07:37.202298Z",
     "execution_failed": "2026-02-08T05:08:01.601Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY\")\nprint(\"=\"*60)\nfor r in fold_results:\n    print(f\"Fold {r['fold']+1}: Best Balanced Acc = {100*r['best_balanced_acc']:.1f}%\")",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Final Evaluation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load best model and evaluate\nmodel.eval()\nall_preds = []\nall_labels = []\nall_probs = []",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "with torch.no_grad():\n    for images, labels, level_idx in val_loader:\n        images = images.to(CONFIG['device'])\n        level_idx = level_idx.to(CONFIG['device'])\n        \n        with autocast('cuda'):\n            outputs, _ = model(images, level_idx)\n            probs = F.softmax(outputs, dim=1)\n        \n        _, predicted = torch.max(outputs, 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n        all_probs.extend(probs.cpu().numpy())",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "all_preds = np.array(all_preds)\nall_labels = np.array(all_labels)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*50)\nprint(\"CLASSIFICATION REPORT\")\nprint(\"=\"*50)\nprint(classification_report(all_labels, all_preds, \n                           target_names=['Normal/Mild', 'Moderate', 'Severe']))",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE - Version 4 (FIXED)\")\nprint(\"=\"*60)\nprint(f\"\\nKey Fixes in v4:\")\nprint(f\"  ‚úì Class weights from ORIGINAL data (not oversampled!)\")\nprint(f\"  ‚úì Model selection based on BALANCED ACCURACY (not val_loss!)\")\nprint(f\"  ‚úì Increased focal_gamma to {CONFIG['focal_gamma']} (from 2.0)\")\nprint(f\"  ‚úì Stronger augmentations for oversampled samples\")\nprint(f\"  ‚úì Minimum minority recall threshold for model saving\")\nprint(f\"\\nüéØ These changes should significantly improve Moderate and Severe recall!\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ]
}