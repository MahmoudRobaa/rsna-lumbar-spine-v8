{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA 2024 ‚Äî Version 10\n",
    "## v4 Faithful Core + Minimal Improvements\n",
    "\n",
    "**Philosophy:** Keep v4's proven system (74.9% BA). Change only 3 things with strong evidence.\n",
    "\n",
    "| Component | v4 (74.9%) | v10 | Changed? |\n",
    "|-----------|-----------|-----|----------|\n",
    "| RNN | LSTM bidir | LSTM bidir | ‚úÖ Same |\n",
    "| Attention | MultiheadAttention(4) | MultiheadAttention(4) | ‚úÖ Same |\n",
    "| Oversampling | Progressive stratified | Progressive stratified | ‚úÖ Same |\n",
    "| Aux heads | Binary + Severity | Binary + Severity | ‚úÖ Same |\n",
    "| Loss | FocalLoss(Œ≥=2.5, LS=0.1) | FocalLoss(Œ≥=2.5, LS=0.1) | ‚úÖ Same |\n",
    "| LR | 3e-4 / 3e-5 | 3e-4 / 3e-5 | ‚úÖ Same |\n",
    "| Crop | img_size//2 | img_size//2 | ‚úÖ Same |\n",
    "| **Weights** | **sqrt-inverse** | **[1, 2, 4] competition** | üîÑ Changed |\n",
    "| **Grad clip** | **None** | **max_norm=1.0** | üîÑ Added |\n",
    "| **Syntax bug** | **Missing comma** | **Fixed** | üêõ Fixed |\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "import os, copy, cv2, glob, pydicom, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'img_size': 256,\n",
    "    'seq_length': 7,\n",
    "    'batch_size': 8,           # v4 exact\n",
    "    'epochs': 25,\n",
    "    'learning_rate': 3e-4,     # v4 exact (NOT 1e-4)\n",
    "    'backbone_lr': 3e-5,       # v4 exact (NOT 1e-5)\n",
    "    'weight_decay': 0.05,      # v4 exact\n",
    "    'patience': 10,\n",
    "    'num_folds': 5,\n",
    "    'train_folds': [0],\n",
    "    'focal_gamma': 2.5,        # v4 exact\n",
    "    'label_smoothing': 0.1,    # v4 exact\n",
    "    'dropout': 0.4,            # v4 exact\n",
    "    'num_attention_heads': 4,  # v4 exact\n",
    "    'warmup_epochs': 2,        # v4 exact\n",
    "    'oversample_strategy': 'progressive',  # v4 exact\n",
    "    'min_minority_recall': 0.20,\n",
    "    'aux_weight': 0.1,         # v4 exact\n",
    "    \n",
    "    # --- CHANGE 1: Competition weights instead of sqrt-inverse ---\n",
    "    'class_weights': [1.0, 2.0, 4.0],\n",
    "    \n",
    "    # --- CHANGE 2: Gradient clipping (v4 had none) ---\n",
    "    'clip_grad_norm': 1.0,\n",
    "    \n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'target_condition': 'spinal_canal_stenosis',\n",
    "    'target_series': 'Sagittal T2/STIR'\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CONFIG['seed'])\n",
    "print(f\"‚úÖ v10: v4 Faithful Core + Competition Weights + Grad Clip\")\n",
    "print(f\"   LR: {CONFIG['learning_rate']}/{CONFIG['backbone_lr']} (v4 exact)\")\n",
    "print(f\"   Focal: Œ≥={CONFIG['focal_gamma']}, LS={CONFIG['label_smoothing']} (v4 exact)\")\n",
    "print(f\"   Weights: {CONFIG['class_weights']} (competition, was sqrt-inverse)\")\n",
    "print(f\"   Grad clip: {CONFIG['clip_grad_norm']} (new safety net)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "DATA_ROOT = \"/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\"\n",
    "TRAIN_IMAGES = os.path.join(DATA_ROOT, \"train_images\")\n",
    "\n",
    "df_train = pd.read_csv(f\"{DATA_ROOT}/train.csv\")\n",
    "df_coords = pd.read_csv(f\"{DATA_ROOT}/train_label_coordinates.csv\")\n",
    "df_desc = pd.read_csv(f\"{DATA_ROOT}/train_series_descriptions.csv\")\n",
    "\n",
    "df_train.columns = [col.lower().replace('/', '_') for col in df_train.columns]\n",
    "condition_cols = [c for c in df_train.columns if c != 'study_id']\n",
    "df_labels = pd.melt(df_train, id_vars=['study_id'], value_vars=condition_cols,\n",
    "                    var_name='condition_level', value_name='severity')\n",
    "df_labels = df_labels.dropna(subset=['severity'])\n",
    "df_labels['severity'] = df_labels['severity'].astype(str).str.lower().str.replace('/', '_')\n",
    "\n",
    "def extract_meta(val):\n",
    "    parts = val.split('_')\n",
    "    level = parts[-2] + '_' + parts[-1]\n",
    "    condition = '_'.join(parts[:-2])\n",
    "    return condition, level\n",
    "\n",
    "df_labels[['base_condition', 'level_str']] = df_labels['condition_level'].apply(lambda x: pd.Series(extract_meta(x)))\n",
    "severity_map = {'normal_mild': 0, 'moderate': 1, 'severe': 2}\n",
    "df_labels['label'] = df_labels['severity'].map(severity_map)\n",
    "df_labels = df_labels.dropna(subset=['label'])\n",
    "df_labels['label'] = df_labels['label'].astype(int)\n",
    "\n",
    "df_coords = df_coords.merge(df_desc, on=['study_id', 'series_id'], how='left')\n",
    "df_coords['condition'] = df_coords['condition'].str.lower().str.replace(' ', '_')\n",
    "df_coords['level'] = df_coords['level'].str.lower().str.replace('/', '_')\n",
    "df_coords['condition_level'] = df_coords['condition'] + '_' + df_coords['level']\n",
    "\n",
    "df_model = df_labels[df_labels['base_condition'] == CONFIG['target_condition']].copy()\n",
    "df_coords_filt = df_coords[(df_coords['condition'] == CONFIG['target_condition']) & \n",
    "                           (df_coords['series_description'] == CONFIG['target_series'])]\n",
    "\n",
    "df_final = df_model.merge(df_coords_filt[['study_id', 'condition_level', 'series_id', 'instance_number', 'x', 'y']],\n",
    "                          on=['study_id', 'condition_level'], how='inner')\n",
    "\n",
    "valid_rows = []\n",
    "for index, row in tqdm(df_final.iterrows(), total=len(df_final), desc=\"Checking Files\"):\n",
    "    path = f\"{TRAIN_IMAGES}/{row['study_id']}/{row['series_id']}/{int(row['instance_number'])}.dcm\"\n",
    "    if os.path.exists(path):\n",
    "        valid_rows.append(row)\n",
    "\n",
    "df_final = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
    "level_map = {'l1_l2': 0, 'l2_l3': 1, 'l3_l4': 2, 'l4_l5': 3, 'l5_s1': 4}\n",
    "df_final['level_idx'] = df_final['level_str'].map(level_map)\n",
    "\n",
    "print(f\"\\n‚úÖ Data: {len(df_final)} samples\")\n",
    "for i in range(3):\n",
    "    c = (df_final['label']==i).sum()\n",
    "    print(f\"   Class {i}: {c} ({100*c/len(df_final):.1f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Progressive Oversampling (v4 ‚Äî proven)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "def create_stratified_balanced_df(df, strategy='progressive', random_state=42):\n",
    "    \"\"\"\n",
    "    v4's progressive balancing: considers both label AND level together.\n",
    "    Adds augmentation variant tracking for diversity.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    grouped = df.groupby(['level_idx', 'label'])\n",
    "    balanced_dfs = []\n",
    "    \n",
    "    print(\"\\nüìä Stratified Sampling Details:\")\n",
    "    \n",
    "    for (level, label), group_df in grouped:\n",
    "        group_df = group_df.copy()\n",
    "        group_df['is_oversampled'] = False\n",
    "        group_df['aug_variant'] = 0\n",
    "        current_count = len(group_df)\n",
    "        level_counts = df[df['level_idx'] == level]['label'].value_counts()\n",
    "        \n",
    "        if strategy == 'progressive':\n",
    "            target_count = int(level_counts.median() * (1 + 0.3 * label))\n",
    "        elif strategy == 'balanced':\n",
    "            target_count = level_counts.max()\n",
    "        else:\n",
    "            target_count = current_count\n",
    "        \n",
    "        samples_needed = target_count - current_count\n",
    "        \n",
    "        if samples_needed > 0:\n",
    "            oversample_indices = np.random.choice(group_df.index, size=samples_needed, replace=True)\n",
    "            oversampled_df = df.loc[oversample_indices].copy()\n",
    "            oversampled_df['is_oversampled'] = True\n",
    "            oversampled_df['aug_variant'] = np.random.randint(0, 4, size=len(oversampled_df))\n",
    "            print(f\"   Level {level}, Label {label}: {current_count} ‚Üí {target_count} (+{samples_needed})\")\n",
    "            balanced_dfs.append(group_df)\n",
    "            balanced_dfs.append(oversampled_df)\n",
    "        else:\n",
    "            print(f\"   Level {level}, Label {label}: {current_count} (no oversampling)\")\n",
    "            balanced_dfs.append(group_df)\n",
    "    \n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    return balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "print(\"‚úÖ Progressive oversampling (v4 exact)\")\n",
    "print(\"   Minority classes get MORE copies, with STRONGER augmentation\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset (v4 ‚Äî with Adaptive Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "class RSNASequenceDataset(Dataset):\n",
    "    \"\"\"v4's exact dataset: min-max DICOM, CLAHE, crop_size=img_size//2, dual aug.\"\"\"\n",
    "    def __init__(self, df, seq_length=7, img_size=256, transform=None, \n",
    "                 strong_transform=None, is_training=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.seq_length = seq_length\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.strong_transform = strong_transform\n",
    "        self.is_training = is_training\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def load_dicom(self, path):\n",
    "        try:\n",
    "            dcm = pydicom.dcmread(path)\n",
    "            img = dcm.pixel_array.astype(np.float32)\n",
    "            if img.max() > img.min():\n",
    "                img = (img - img.min()) / (img.max() - img.min()) * 255.0\n",
    "            else:\n",
    "                img = np.zeros_like(img)\n",
    "            img = img.astype(np.uint8)\n",
    "            img = self.clahe.apply(img)\n",
    "            return img\n",
    "        except:\n",
    "            return np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        center_inst = int(row['instance_number'])\n",
    "        study_path = f\"{TRAIN_IMAGES}/{row['study_id']}/{row['series_id']}\"\n",
    "        cx, cy = int(row['x']), int(row['y'])\n",
    "        \n",
    "        is_oversampled = row.get('is_oversampled', False)\n",
    "        aug_variant = row.get('aug_variant', 0)\n",
    "        \n",
    "        if is_oversampled and self.is_training:\n",
    "            np.random.seed(idx * 1000 + int(aug_variant))\n",
    "            random.seed(idx * 1000 + int(aug_variant))\n",
    "        \n",
    "        start = center_inst - (self.seq_length // 2)\n",
    "        indices = [start + i for i in range(self.seq_length)]\n",
    "        \n",
    "        images_list = []\n",
    "        for inst in indices:\n",
    "            path = os.path.join(study_path, f\"{inst}.dcm\")\n",
    "            if os.path.exists(path):\n",
    "                img = self.load_dicom(path)\n",
    "            else:\n",
    "                img = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n",
    "            \n",
    "            h, w = img.shape\n",
    "            crop_size = self.img_size // 2   # v4 exact: 128px from center\n",
    "            x1 = max(0, cx - crop_size)\n",
    "            y1 = max(0, cy - crop_size)\n",
    "            x2 = min(w, cx + crop_size)\n",
    "            y2 = min(h, cy + crop_size)\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            \n",
    "            if crop.size == 0:\n",
    "                crop = np.zeros((self.img_size, self.img_size), dtype=np.uint8)\n",
    "            else:\n",
    "                crop = cv2.resize(crop, (self.img_size, self.img_size))\n",
    "            \n",
    "            crop = cv2.cvtColor(crop, cv2.COLOR_GRAY2RGB)\n",
    "            \n",
    "            if self.is_training and is_oversampled and self.strong_transform:\n",
    "                res = self.strong_transform(image=crop)\n",
    "            elif self.transform:\n",
    "                res = self.transform(image=crop)\n",
    "            else:\n",
    "                res = {'image': torch.tensor(crop).permute(2, 0, 1).float() / 255.0}\n",
    "            \n",
    "            images_list.append(res['image'])\n",
    "            \n",
    "        sequence = torch.stack(images_list, dim=0)\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        level_idx = torch.tensor(row['level_idx'], dtype=torch.long)\n",
    "        \n",
    "        return sequence, label, level_idx\n",
    "\n",
    "print(\"‚úÖ Dataset (v4 exact)\")\n",
    "print(\"   DICOM: min-max normalize ‚Üí CLAHE\")\n",
    "print(\"   Crop: img_size//2 = 128px from center\")\n",
    "print(\"   Oversampled copies ‚Üí strong augmentation\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Augmentation (v4 ‚Äî Normal + Strong)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "# Normal augmentation (for original samples)\n",
    "train_aug = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=5, \n",
    "                       border_mode=cv2.BORDER_CONSTANT, value=0, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(var_limit=(5.0, 20.0), p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Strong augmentation (for oversampled minority copies ‚Äî prevents memorization)\n",
    "strong_aug = A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=8,\n",
    "                       border_mode=cv2.BORDER_CONSTANT, value=0, p=0.7),\n",
    "    A.ElasticTransform(alpha=1, sigma=50, p=0.3),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1.0),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "        A.CLAHE(clip_limit=4.0, p=1.0),\n",
    "    ], p=0.8),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "        A.MultiplicativeNoise(multiplier=(0.9, 1.1), p=1.0),\n",
    "    ], p=0.3),\n",
    "    A.GridDistortion(num_steps=5, distort_limit=0.1, p=0.3),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_aug = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Dual augmentation (v4 exact)\")\n",
    "print(\"   Normal: conservative (original samples)\")\n",
    "print(\"   Strong: aggressive (oversampled copies ‚Äî Elastic, Grid, CLAHE)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture (v4 ‚Äî with Aux Heads)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "class SpineModelV10(nn.Module):\n",
    "    \"\"\"\n",
    "    v4's exact architecture with fixed syntax.\n",
    "    \n",
    "    LSTM (not GRU) + MultiheadAttention + Aux Heads\n",
    "    This is what achieved 74.9% BA.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=3, hidden_dim=256, lstm_layers=2, \n",
    "                 num_heads=4, dropout=0.4, num_levels=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone: EfficientNet-V2-S (v4 exact)\n",
    "        effnet = models.efficientnet_v2_s(weights='IMAGENET1K_V1')\n",
    "        self.backbone = nn.Sequential(*list(effnet.children())[:-1]) \n",
    "        self.feature_dim = 1280 \n",
    "        \n",
    "        # Feature projection (v4 exact: Dropout‚ÜíLinear‚ÜíLN‚ÜíGELU)\n",
    "        self.feature_proj = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.feature_dim, hidden_dim * 2),\n",
    "            nn.LayerNorm(hidden_dim * 2),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # LSTM bidirectional (v4 exact ‚Äî NOT GRU)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim * 2, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=lstm_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # MultiheadAttention (v4 exact ‚Äî NOT custom MIL)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Level embedding dim=64 (v4 exact)\n",
    "        self.level_embedding = nn.Embedding(num_levels, 64)\n",
    "        \n",
    "        context_dim = hidden_dim * 2 + 64  # 512 + 64 = 576\n",
    "        \n",
    "        # Main 3-class classifier (v4 exact)\n",
    "        self.main_classifier = nn.Sequential(\n",
    "            nn.LayerNorm(context_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(context_dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Auxiliary binary classifier: normal vs abnormal (v4 exact)\n",
    "        self.aux_binary = nn.Sequential(\n",
    "            nn.Linear(context_dim, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "        # Auxiliary severity classifier: moderate vs severe (v4 exact)\n",
    "        self.aux_severity = nn.Sequential(\n",
    "            nn.Linear(context_dim, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, level_idx=None):\n",
    "        b, s, c, h, w = x.size()\n",
    "        x = x.view(b * s, c, h, w)\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        features = features.view(b, s, -1)\n",
    "        features = self.feature_proj(features)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        context = attn_out.mean(dim=1)  # v4 exact: mean pooling\n",
    "        \n",
    "        if level_idx is not None:\n",
    "            level_feat = self.level_embedding(level_idx)\n",
    "            context = torch.cat([context, level_feat], dim=-1)\n",
    "        else:\n",
    "            context = torch.cat([context, torch.zeros(b, 64, device=x.device)], dim=-1)\n",
    "        \n",
    "        main_out = self.main_classifier(context)\n",
    "        binary_out = self.aux_binary(context)\n",
    "        severity_out = self.aux_severity(context)\n",
    "        \n",
    "        return {\n",
    "            'main': main_out,\n",
    "            'binary': binary_out,\n",
    "            'severity': severity_out,\n",
    "            'attention': attn_weights\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SpineModelV10 (v4 exact architecture)\")\n",
    "print(\"   - LSTM bidirectional 2-layer (hidden=256)\")\n",
    "print(\"   - MultiheadAttention (4 heads) ‚Üí mean pooling\")\n",
    "print(\"   - Aux heads: binary (normal vs abnormal) + severity (mod vs sev)\")\n",
    "print(\"   - Level embedding (dim=64)\")\n",
    "print(\"   - Feature proj: Dropout‚ÜíLinear‚ÜíLN‚ÜíGELU\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function (v4 Focal + Competition Weights)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"v4's exact Focal Loss with label smoothing.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(\n",
    "            inputs, targets, \n",
    "            weight=self.alpha, \n",
    "            reduction='none', \n",
    "            label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss\n",
    "\n",
    "def compute_loss_with_aux(outputs, labels, criterion, aux_weight=0.1):\n",
    "    \"\"\"v4's exact auxiliary loss computation.\"\"\"\n",
    "    main_loss = criterion(outputs['main'], labels)\n",
    "    \n",
    "    binary_labels = (labels > 0).long()\n",
    "    binary_loss = F.cross_entropy(outputs['binary'], binary_labels)\n",
    "    \n",
    "    abnormal_mask = labels > 0\n",
    "    if abnormal_mask.sum() > 0:\n",
    "        severity_labels = (labels[abnormal_mask] - 1)\n",
    "        severity_loss = F.cross_entropy(outputs['severity'][abnormal_mask], severity_labels)\n",
    "    else:\n",
    "        severity_loss = torch.tensor(0.0, device=labels.device)\n",
    "    \n",
    "    total_loss = main_loss + aux_weight * (binary_loss + severity_loss)\n",
    "    \n",
    "    return total_loss, {\n",
    "        'total': total_loss.item(),\n",
    "        'main': main_loss.item(),\n",
    "        'binary': binary_loss.item(),\n",
    "        'severity': severity_loss.item()\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ FocalLoss(Œ≥=2.5, label_smoothing=0.1) + Auxiliary losses\")\n",
    "print(\"   Main: 3-class focal with competition weights [1, 2, 4]\")\n",
    "print(\"   Aux binary: normal vs abnormal (CE)\")\n",
    "print(\"   Aux severity: moderate vs severe (CE, abnormal samples only)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "def compute_per_class_metrics(preds, labels, num_classes=3):\n",
    "    metrics = {}\n",
    "    for c in range(num_classes):\n",
    "        mask = (labels == c)\n",
    "        if mask.sum() > 0:\n",
    "            metrics[f'class_{c}_recall'] = ((preds == c) & mask).sum() / mask.sum()\n",
    "        else:\n",
    "            metrics[f'class_{c}_recall'] = 0.0\n",
    "    return metrics\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.005, mode='max'):\n",
    "        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n",
    "        self.counter, self.best_score = 0, None\n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        improved = (score > self.best_score + self.min_delta) if self.mode == 'max' \\\n",
    "                   else (score < self.best_score - self.min_delta)\n",
    "        if improved:\n",
    "            self.best_score, self.counter = score, 0\n",
    "            return False\n",
    "        self.counter += 1\n",
    "        return self.counter >= self.patience"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "def train_one_fold(model, train_loader, val_loader, fold, config):\n",
    "    # --- CHANGE 1: Competition weights instead of sqrt-inverse ---\n",
    "    loss_weights = torch.FloatTensor(config['class_weights']).to(config['device'])\n",
    "    print(f\"\\n   üìä Loss weights: {config['class_weights']} (competition metric aligned)\")\n",
    "    \n",
    "    criterion = FocalLoss(\n",
    "        alpha=loss_weights, \n",
    "        gamma=config['focal_gamma'], \n",
    "        label_smoothing=config['label_smoothing']\n",
    "    )\n",
    "    \n",
    "    # v4 exact optimizer setup\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.backbone.parameters(), 'lr': config['backbone_lr']},\n",
    "        {'params': model.feature_proj.parameters(), 'lr': config['learning_rate']},\n",
    "        {'params': model.lstm.parameters(), 'lr': config['learning_rate']},\n",
    "        {'params': model.attention.parameters(), 'lr': config['learning_rate']},\n",
    "        {'params': model.level_embedding.parameters(), 'lr': config['learning_rate']},\n",
    "        {'params': model.main_classifier.parameters(), 'lr': config['learning_rate']},\n",
    "        {'params': model.aux_binary.parameters(), 'lr': config['learning_rate']},\n",
    "        {'params': model.aux_severity.parameters(), 'lr': config['learning_rate']}\n",
    "    ], weight_decay=config['weight_decay'])\n",
    "    \n",
    "    # v4 exact scheduler\n",
    "    warmup_steps = config['warmup_epochs'] * len(train_loader)\n",
    "    total_steps = config['epochs'] * len(train_loader)\n",
    "    \n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / max(warmup_steps, 1)\n",
    "        progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "        return max(0.5 * (1 + np.cos(np.pi * progress)), 1e-6)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    scaler = GradScaler('cuda')\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=config['patience'], min_delta=0.005, mode='max')\n",
    "    best_ba = 0.0\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [],\n",
    "        'balanced_acc': [], 'class_0_recall': [], 'class_1_recall': [], 'class_2_recall': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüöÄ Fold {fold+1} Training (v10 = v4 core + competition weights)\")\n",
    "    print(f\"   Train: {len(train_loader.dataset)}, Val: {len(val_loader.dataset)}\")\n",
    "    print(f\"   Focal Œ≥={config['focal_gamma']}, LS={config['label_smoothing']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}/{config['backbone_lr']} (v4 exact)\")\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        for images, labels, level_idx in loop:\n",
    "            images = images.to(config['device'])\n",
    "            labels = labels.to(config['device'])\n",
    "            level_idx = level_idx.to(config['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                outputs = model(images, level_idx)\n",
    "                loss, loss_dict = compute_loss_with_aux(\n",
    "                    outputs, labels, criterion, aux_weight=config['aux_weight']\n",
    "                )\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # --- CHANGE 2: Gradient clipping (v4 had none) ---\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config['clip_grad_norm'])\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            preds = outputs['main'].argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loop.set_postfix(\n",
    "                loss=f\"{train_loss/(loop.n+1):.4f}\",\n",
    "                acc=f\"{100*correct/total:.1f}%\"\n",
    "            )\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, level_idx in val_loader:\n",
    "                images = images.to(config['device'])\n",
    "                labels = labels.to(config['device'])\n",
    "                level_idx = level_idx.to(config['device'])\n",
    "                \n",
    "                with autocast('cuda'):\n",
    "                    outputs = model(images, level_idx)\n",
    "                    loss, _ = compute_loss_with_aux(\n",
    "                        outputs, labels, criterion, aux_weight=config['aux_weight']\n",
    "                    )\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                preds = outputs['main'].argmax(dim=1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        all_preds, all_labels = np.array(all_preds), np.array(all_labels)\n",
    "        pc = compute_per_class_metrics(all_preds, all_labels)\n",
    "        ba = (pc['class_0_recall'] + pc['class_1_recall'] + pc['class_2_recall']) / 3\n",
    "        \n",
    "        # Dead class monitor\n",
    "        pred_counts = np.bincount(all_preds, minlength=3)\n",
    "        if pred_counts.min() < 3:\n",
    "            print(f\"   ‚ö†Ô∏è Dead class warning: prediction counts = {pred_counts.tolist()}\")\n",
    "        \n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss / len(val_loader))\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['balanced_acc'].append(ba)\n",
    "        for c in range(3):\n",
    "            history[f'class_{c}_recall'].append(pc[f'class_{c}_recall'])\n",
    "        \n",
    "        print(f\"üìä Train: {100*train_acc:.1f}% | Val: {100*val_acc:.1f}% | \"\n",
    "              f\"N={100*pc['class_0_recall']:.1f}% M={100*pc['class_1_recall']:.1f}% \"\n",
    "              f\"S={100*pc['class_2_recall']:.1f}% | BA={100*ba:.1f}%\")\n",
    "        \n",
    "        min_minority = min(pc['class_1_recall'], pc['class_2_recall'])\n",
    "        if ba > best_ba and min_minority >= config['min_minority_recall']:\n",
    "            best_ba = ba\n",
    "            torch.save(model.state_dict(), f\"best_v10_fold{fold}.pth\")\n",
    "            print(f\"   ‚úÖ Saved! BA={100*ba:.1f}% (min minority={100*min_minority:.1f}%)\")\n",
    "        \n",
    "        if early_stopping(ba):\n",
    "            print(f\"   ‚èπÔ∏è Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"best_v10_fold{fold}.pth\"))\n",
    "    return model, history, best_ba"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "kfold = StratifiedGroupKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(df_final, df_final['label'], df_final['study_id'])):\n",
    "    if fold not in CONFIG['train_folds']:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold+1} (v10 = v4 core + competition weights + grad clip)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    train_df_original = df_final.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = df_final.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Show original class distribution\n",
    "    original_counts = np.bincount(train_df_original['label'].values, minlength=3)\n",
    "    print(f\"\\nüìä ORIGINAL class distribution:\")\n",
    "    for i, c in enumerate(original_counts):\n",
    "        print(f\"   Class {i}: {c} ({100*c/len(train_df_original):.1f}%)\")\n",
    "    \n",
    "    # Apply v4's progressive oversampling\n",
    "    train_df = create_stratified_balanced_df(\n",
    "        train_df_original, \n",
    "        strategy=CONFIG['oversample_strategy'],\n",
    "        random_state=CONFIG['seed'] + fold\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä After oversampling: {len(train_df_original)} ‚Üí {len(train_df)} samples\")\n",
    "    print(f\"   New distribution: {train_df['label'].value_counts().sort_index().to_dict()}\")\n",
    "    \n",
    "    train_ds = RSNASequenceDataset(\n",
    "        train_df, seq_length=CONFIG['seq_length'], img_size=CONFIG['img_size'],\n",
    "        transform=train_aug, strong_transform=strong_aug, is_training=True\n",
    "    )\n",
    "    \n",
    "    val_df['is_oversampled'] = False\n",
    "    val_ds = RSNASequenceDataset(\n",
    "        val_df, seq_length=CONFIG['seq_length'], img_size=CONFIG['img_size'],\n",
    "        transform=val_aug, is_training=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "                             num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "                           num_workers=2, pin_memory=True)\n",
    "    \n",
    "    model = SpineModelV10(\n",
    "        num_classes=3,\n",
    "        num_heads=CONFIG['num_attention_heads'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(CONFIG['device'])\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"   üèóÔ∏è SpineModelV10: {total_params:,} params\")\n",
    "    \n",
    "    model, history, best_ba = train_one_fold(model, train_loader, val_loader, fold, CONFIG)\n",
    "    fold_results.append({'fold': fold, 'best_ba': best_ba, 'history': history})\n",
    "    print(f\"\\n‚úÖ Fold {fold+1}: Best BA = {100*best_ba:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for r in fold_results:\n",
    "    print(f\"Fold {r['fold']+1}: BA = {100*r['best_ba']:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "model.eval()\n",
    "all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, level_idx in val_loader:\n",
    "        images = images.to(CONFIG['device'])\n",
    "        level_idx = level_idx.to(CONFIG['device'])\n",
    "        \n",
    "        with autocast('cuda'):\n",
    "            outputs = model(images, level_idx)\n",
    "            probs = F.softmax(outputs['main'], dim=1)\n",
    "        \n",
    "        preds = outputs['main'].argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(all_labels, all_preds,\n",
    "                           target_names=['Normal/Mild', 'Moderate', 'Severe']))\n",
    "\n",
    "pc = compute_per_class_metrics(all_preds, all_labels)\n",
    "ba = np.mean([pc[f'class_{c}_recall'] for c in range(3)])\n",
    "print(f\"\\nüéØ Final Balanced Accuracy: {100*ba:.1f}%\")\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=['Normal/Mild', 'Moderate', 'Severe'],\n",
    "            yticklabels=['Normal/Mild', 'Moderate', 'Severe'])\n",
    "plt.ylabel('True'); plt.xlabel('Predicted')\n",
    "plt.title(f'v10 Confusion Matrix (BA: {100*ba:.1f}%)')\n",
    "plt.tight_layout(); plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training History"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "if fold_results:\n",
    "    h = fold_results[0]['history']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    ep = range(1, len(h['train_loss'])+1)\n",
    "    \n",
    "    axes[0].plot(ep, h['train_loss'], 'b-', label='Train')\n",
    "    axes[0].plot(ep, h['val_loss'], 'r-', label='Val')\n",
    "    axes[0].set_title('Loss'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(ep, h['class_0_recall'], 'g-o', label='Normal', ms=3)\n",
    "    axes[1].plot(ep, h['class_1_recall'], color='orange', marker='s', label='Moderate', ms=3)\n",
    "    axes[1].plot(ep, h['class_2_recall'], 'r-^', label='Severe', ms=3)\n",
    "    axes[1].axhline(y=0.75, color='gray', linestyle='--', alpha=0.3)\n",
    "    axes[1].set_title('Per-Class Recall'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(ep, h['balanced_acc'], 'purple', marker='d', lw=2, ms=3)\n",
    "    axes[2].axhline(y=0.749, color='red', linestyle='--', alpha=0.5, label='v4 Best (74.9%)')\n",
    "    axes[2].set_title(f'BA (Best: {100*max(h[\"balanced_acc\"]):.1f}%)')\n",
    "    axes[2].legend(); axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('v10: v4 Core + Competition Weights + Grad Clip')\n",
    "    plt.tight_layout(); plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### v10 = v4 Faithful Core + 3 Targeted Improvements\n",
    "\n",
    "**Kept from v4 (every component that produced 74.9% BA):**\n",
    "- ‚úÖ LSTM bidirectional 2-layer\n",
    "- ‚úÖ MultiheadAttention (4 heads) ‚Üí mean pooling\n",
    "- ‚úÖ Progressive oversampling (stratified by level √ó label)\n",
    "- ‚úÖ Dual augmentation (normal + strong for oversampled)\n",
    "- ‚úÖ Auxiliary heads (binary + severity)\n",
    "- ‚úÖ FocalLoss (Œ≥=2.5, label_smoothing=0.1)\n",
    "- ‚úÖ LR: 3e-4 / 3e-5 (v4 exact)\n",
    "- ‚úÖ Crop: img_size//2 (v4 exact)\n",
    "- ‚úÖ DICOM: min-max ‚Üí CLAHE (v4 exact)\n",
    "- ‚úÖ Batch size 8 (v4 exact)\n",
    "\n",
    "**3 targeted changes:**\n",
    "1. üîÑ Competition weights [1, 2, 4] (instead of sqrt-inverse)\n",
    "2. üîÑ Gradient clipping max_norm=1.0 (safety net)\n",
    "3. üêõ Fixed syntax error in model init\n"
   ]
  }
 ]
}